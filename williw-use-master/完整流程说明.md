# 完整流程说明

## 流程概述

本系统实现了用户节点（需要部署模型的节点）的完整工作流程，包括模型下载、元数据生成、Worker 算法处理和模型切分。

## 完整流程步骤

```
用户节点                          Worker                    Hugging Face
   |                                |                            |
   |--1. 发送模型名称请求----------->|                            |
   |<--确认接收---------------------|                            |
   |                                |                            |
   |--2. 下载模型-------------------|--------------------------->|
   |<--模型文件---------------------|                            |
   |                                |                            |
   |--3. 提取 state_dict            |                            |
   |   生成元数据                   |                            |
   |                                |                            |
   |--4. 上传元数据-----------------|--------------------------->|
   |                                |                            |
   |--5. 通知 Worker 处理----------->|                            |
   |                                |--读取元数据--------------->|
   |                                |                            |
   |                                |--执行算法（节点选择、      |
   |                                |  按算力切分、Megaphone）   |
   |                                |                            |
   |<--6. 接收分配方案---------------|                            |
   |                                |                            |
   |--7. 根据方案切分和分发          |                            |
```

## 详细步骤说明

### 步骤1: 用户节点发送模型名称给 Worker

**用户节点端：**
```python
client._send_model_request(model_name)
# POST /api/request
# {
#   "model_name": "meta-llama/Llama-3.2-1B-Instruct",
#   "node_id": "node_001"
# }
```

**Worker 端：**
- 接收请求，只返回确认
- 不立即处理，等待元数据上传

### 步骤2: 用户节点从 Hugging Face 下载模型

**用户节点端：**
```python
model_path = client._download_model(model_name)
# 使用 transformers.AutoModel.from_pretrained()
# 下载到本地缓存目录
```

**Rust 模块：** `model-downloader`
- 从 Hugging Face 下载模型文件
- 支持 safetensors、bin、config.json 等

### 步骤3: 用户节点提取 state_dict 并生成元数据

**用户节点端：**
```python
metadata = client._generate_metadata(model_name, model_path, batch_size, sequence_length)
# 1. 加载模型，获取 state_dict
# 2. 对每一层进行算力评估
# 3. 生成包含每层算力需求的元数据 JSON
```

**元数据格式：**
```json
{
  "model_name": "meta-llama/Llama-3.2-1B-Instruct",
  "model_type": "transformer",
  "layers": [
    {
      "name": "transformer.h.0.attn.q_proj.weight",
      "num_params": 16777216,
      "compute_required": 134.22,
      "layer_type": "attention"
    }
  ],
  "total_compute": 1234.56
}
```

**Rust 模块：** `metadata-generator`
- 调用 Python 脚本提取 state_dict
- 生成包含每层算力评估的元数据

### 步骤4: 用户节点上传元数据到 Hugging Face 公共仓库

**用户节点端：**
```python
upload_result = client._upload_metadata(filename, metadata)
# 上传到指定的 Hugging Face 公共仓库
```

**Rust 模块：** `metadata-uploader`
- 上传元数据 JSON 到 Hugging Face
- 支持自定义提交信息

### 步骤5: 用户节点通知 Worker 元数据已准备好

**用户节点端：**
```python
process_result = client._trigger_worker_process(model_name)
# POST /api/process
# {
#   "model_name": "meta-llama/Llama-3.2-1B-Instruct",
#   "node_id": "node_001",
#   "metadata_repo": "your-username/model-metadata"
# }
```

**Worker 端：**
1. 从 Hugging Face 读取元数据
2. 获取可用节点列表
3. 执行算法：
   - 节点选择（按算力需求）
   - **按算力切分**（贪心算法，将算力需求高的层分配给算力高的节点）
   - 生成 Megaphone Mode 计划
4. 返回分配方案

### 步骤6: 用户节点接收 Worker 的分配方案

**分配方案格式：**
```json
{
  "success": true,
  "model_name": "meta-llama/Llama-3.2-1B-Instruct",
  "split_plan": {
    "node_001": {
      "layer_names": ["layer1", "layer2"],
      "total_compute": 100.0,
      "compute_utilization": 0.5
    }
  },
  "megaphone_plan": {...}
}
```

### 步骤7: 用户节点根据方案切分和分发模型

**用户节点端：**
```python
split_result = client._execute_split(model_path, split_plan)
# 1. 加载模型 state_dict
# 2. 根据分配方案提取本节点的层
# 3. 保存分片到本地
```

**Rust 模块：** `model-splitter`
- 根据 Worker 的分配方案切分模型
- 支持按算力切分（每层都有算力评估）

## Rust 模块说明

### 1. model-downloader（下载模型）

**功能：** 从 Hugging Face 下载模型文件

**使用示例：**
```rust
use model_downloader::{ModelDownloader, DownloadConfig};

let downloader = ModelDownloader::new(Some("hf_token".to_string()));
let config = DownloadConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    cache_dir: Some("./cache".to_string()),
    hf_token: Some("hf_token".to_string()),
};

let result = downloader.download_model(config).await?;
```

### 2. metadata-generator（生成元数据）

**功能：** 提取 state_dict 并生成包含每层算力评估的元数据

**使用示例：**
```rust
use metadata_generator::{MetadataGenerator, MetadataConfig};

let generator = MetadataGenerator::new();
let config = MetadataConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    model_path: "./cache".to_string(),
    batch_size: 1,
    sequence_length: 512,
    node_id: Some("node_001".to_string()),
};

let metadata = generator.generate_metadata(config).await?;
generator.save_metadata(&metadata, "./metadata.json").await?;
```

### 3. metadata-uploader（上传元数据）

**功能：** 上传元数据到 Hugging Face 公共仓库

**使用示例：**
```rust
use metadata_uploader::{MetadataUploader, UploadConfig};

let uploader = MetadataUploader::new();
let config = UploadConfig {
    metadata_file: "./metadata.json".to_string(),
    repo_id: "your-username/model-metadata".to_string(),
    hf_token: "hf_token".to_string(),
    commit_message: Some("Upload metadata".to_string()),
};

let result = uploader.upload_metadata(config).await?;
```

### 4. model-splitter（按算力切分）

**功能：** 根据 Worker 的分配方案切分模型（按每层的算力切分）

**使用示例：**
```rust
use model_splitter::{ModelSplitter, SplitConfig, SplitPlan};
use std::collections::HashMap;

let splitter = ModelSplitter::new();
let mut split_plan = HashMap::new();
split_plan.insert("node_001".to_string(), SplitPlan {
    node_id: "node_001".to_string(),
    layer_names: vec!["layer1".to_string(), "layer2".to_string()],
    total_compute: 100.0,
    compute_utilization: 0.5,
});

let config = SplitConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    model_path: "./cache".to_string(),
    split_plan,
    output_dir: Some("./shards".to_string()),
};

let result = splitter.split_model(config, "node_001").await?;
```

## 关键特性

### 1. 按算力切分算法

Worker 使用贪心算法进行按算力切分：

1. **按算力需求排序层**：将算力需求高的层排在前面
2. **贪心分配**：将算力需求高的层分配给算力高的节点
3. **确保负载均衡**：确保每个节点的总算力需求不超过其可用算力

### 2. 每层算力评估

元数据中包含每一层的算力评估：

- **层类型识别**：attention、linear、conv2d、layernorm 等
- **算力计算**：基于参数数量、层类型、batch_size、sequence_length
- **Transformer 特殊处理**：考虑序列长度的影响

### 3. 完整流程自动化

从模型请求到切分完成，全流程自动化，无需人工干预。

## 使用方式

### Python 端（用户节点）

```bash
export WORKER_URL="https://your-worker.workers.dev"
export HF_TOKEN="your_huggingface_token"
export METADATA_REPO="your-username/model-metadata"
export NODE_ID="node_001"

python node_client/complete_workflow.py meta-llama/Llama-3.2-1B-Instruct
```

### Worker 端

```bash
cd worker
wrangler deploy
```

### Rust 模块

每个模块都可以独立编译和使用，详见 `rust_modules/README.md`

## 注意事项

1. **Hugging Face Token**：需要有效的 HF Token 才能上传元数据
2. **元数据仓库**：需要先在 Hugging Face 上创建公共仓库
3. **Python 依赖**：Rust 模块中的 Python 脚本需要安装 transformers 和 torch
4. **Worker 配置**：需要配置正确的 METADATA_REPO 变量
