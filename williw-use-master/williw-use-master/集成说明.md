# Williw-Use 项目集成说明

## 项目结构

整合三个项目：
1. **lkc** (`/work/lkc/youhua/lkc`) - 算法层
2. **williw-master** (`/work/lkc/youhua/williw-master`) - 接口层（Rust节点信息）
3. **边缘服务器** - 需要创建（Python Flask API）

## 集成方案

由于lkc项目的核心文件被删除，需要在williw-use项目中重新创建或引用：

### 方案1：直接在williw-use中实现所有模块
- 复用lkc的算法（dcaco_algorithm.py）
- 重新创建核心数据结构
- 整合williw-master的节点信息（通过API或模拟）

### 方案2：引用lkc项目
- 如果lkc项目可以恢复，直接引用
- 否则重新创建核心模块

## 实现步骤

1. ✅ 创建项目结构
2. 创建边缘服务器API
3. 创建模型获取和转换模块
4. 创建算力估算模块（保守估算）
5. 创建节点信息获取接口（从williw-master）
6. 创建分布式推理引擎
7. 创建工作流编排器
8. 创建客户端示例

## 关键模块

### 边缘服务器 (`edge_server/`)
- `api_server.py` - Flask API服务器
- `model_fetcher.py` - 模型获取（Hugging Face/本地）
- `model_converter.py` - ONNX→PyTorch转换
- `compute_estimator.py` - 算力估算（保守，可算多不可算少）
- `workflow_orchestrator.py` - 工作流编排
- `inference_manager.py` - 推理管理

### 接口层 (`interface_layer/`)
- `node_info_api.py` - 节点信息API（从williw-master获取）
- `app_client.py` - 客户端示例

### 模型相关 (`models/`)
- `inference_engine.py` - 分布式推理引擎
- `result_merger.py` - 结果集成

## 工作流程

1. 客户端发送推理请求到边缘服务器
2. 边缘服务器获取模型（HF/本地）
3. 转换为PyTorch，读取state_dict
4. 估算算力需求（保守）
5. 从williw-master获取节点信息
6. 调用lkc算法层：节点选择、路径优化、资源分配、模型切分
7. 执行分布式推理（非训练）
8. 集成结果并返回
