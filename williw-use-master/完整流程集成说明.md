# 完整流程集成说明

## 概述

本文档描述了用户节点完整流程的实现，包括 Python 端完整流程代码、Worker 端代码，以及 4 个 Rust 模块。

## 完整流程

```
用户节点                          Worker                    Hugging Face
   |                                |                            |
   |--1. 发送模型名称请求----------->|                            |
   |<--确认接收---------------------|                            |
   |                                |                            |
   |--2. 下载模型-------------------|--------------------------->|
   |<--模型文件---------------------|                            |
   |                                |                            |
   |--3. 提取 state_dict            |                            |
   |   生成元数据                   |                            |
   |                                |                            |
   |--4. 上传元数据-----------------|--------------------------->|
   |                                |                            |
   |--5. 通知 Worker 处理----------->|                            |
   |                                |--读取元数据--------------->|
   |                                |                            |
   |                                |--执行算法（节点选择、      |
   |                                |  按算力切分、Megaphone）   |
   |                                |                            |
   |<--6. 接收分配方案---------------|                            |
   |                                |                            |
   |--7. 根据方案切分和分发          |                            |
```

## 文件结构

```
williw-use/
├── node_client/
│   └── complete_workflow.py      # 用户节点端完整流程
├── worker/
│   └── src/
│       └── index.js              # Worker 端代码
└── rust_modules/
    ├── model_downloader/          # Rust 模块1: 下载模型
    │   ├── Cargo.toml
    │   └── src/lib.rs
    ├── metadata_generator/        # Rust 模块2: 生成元数据
    │   ├── Cargo.toml
    │   ├── src/lib.rs
    │   └── scripts/
    │       └── generate_metadata.py
    ├── metadata_uploader/          # Rust 模块3: 上传元数据
    │   ├── Cargo.toml
    │   └── src/lib.rs
    └── model_splitter/             # Rust 模块4: 按算力切分
        ├── Cargo.toml
        ├── src/lib.rs
        └── scripts/
            └── split_model.py
```

## 使用说明

### 1. 用户节点端（Python）

#### 安装依赖

```bash
pip install transformers torch huggingface-hub requests
```

#### 配置环境变量

```bash
export WORKER_URL="https://your-worker.workers.dev"
export HF_TOKEN="your_huggingface_token"
export METADATA_REPO="your-username/model-metadata"
export NODE_ID="node_001"
```

#### 运行完整流程

```python
from node_client.complete_workflow import NodeClient

client = NodeClient(
    worker_url=os.getenv("WORKER_URL"),
    hf_token=os.getenv("HF_TOKEN"),
    metadata_repo=os.getenv("METADATA_REPO"),
    node_id=os.getenv("NODE_ID")
)

result = client.execute_complete_workflow(
    model_name="meta-llama/Llama-3.2-1B-Instruct",
    batch_size=1,
    sequence_length=512
)
```

**完整流程步骤：**
1. 发送模型名称给 Worker（只发送请求，不等待处理）
2. 从 Hugging Face 下载模型
3. 提取 state_dict 并生成元数据（包含每层算力评估）
4. 上传元数据到 Hugging Face 公共仓库
5. 通知 Worker 元数据已准备好，触发算法处理
6. 接收 Worker 的分配方案（包含按算力切分的结果）
7. 根据方案切分和分发模型

或使用命令行：

```bash
python node_client/complete_workflow.py meta-llama/Llama-3.2-1B-Instruct
```

### 2. Worker 端（JavaScript）

#### 部署到 Cloudflare Workers

1. 安装 Wrangler CLI：
```bash
npm install -g wrangler
```

2. 配置 `wrangler.toml`：
```toml
name = "williw-worker"
main = "src/index.js"
compatibility_date = "2024-01-01"

[vars]
METADATA_REPO = "your-username/model-metadata"
```

3. 部署：
```bash
wrangler deploy
```

#### API 端点

- `POST /api/request`: 接收模型请求（只接收请求，不处理）
- `POST /api/process`: 用户节点通知 Worker 元数据已准备好，触发算法处理
- `POST /api/get-plan`: 获取分配方案（可选）

### 3. Rust 模块使用

#### 模块1: 下载模型

```rust
use model_downloader::{ModelDownloader, DownloadConfig};

let downloader = ModelDownloader::new(Some("hf_token".to_string()));
let config = DownloadConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    cache_dir: Some("./cache".to_string()),
    hf_token: Some("hf_token".to_string()),
};

let result = downloader.download_model(config).await?;
println!("下载完成: {} MB", result.total_size_mb);
```

#### 模块2: 生成元数据

```rust
use metadata_generator::{MetadataGenerator, MetadataConfig};

let generator = MetadataGenerator::new();
let config = MetadataConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    model_path: "./cache".to_string(),
    batch_size: 1,
    sequence_length: 512,
    node_id: Some("node_001".to_string()),
};

let metadata = generator.generate_metadata(config).await?;
generator.save_metadata(&metadata, "./metadata.json").await?;
```

#### 模块3: 上传元数据

```rust
use metadata_uploader::{MetadataUploader, UploadConfig};

let uploader = MetadataUploader::new();
let config = UploadConfig {
    metadata_file: "./metadata.json".to_string(),
    repo_id: "your-username/model-metadata".to_string(),
    hf_token: "hf_token".to_string(),
    commit_message: Some("Upload metadata".to_string()),
};

let result = uploader.upload_metadata(config).await?;
println!("上传成功: {}", result.url);
```

#### 模块4: 按算力切分

```rust
use model_splitter::{ModelSplitter, SplitConfig, SplitPlan};
use std::collections::HashMap;

let splitter = ModelSplitter::new();
let mut split_plan = HashMap::new();
split_plan.insert("node_001".to_string(), SplitPlan {
    node_id: "node_001".to_string(),
    layer_names: vec!["layer1".to_string(), "layer2".to_string()],
    total_compute: 100.0,
    compute_utilization: 0.5,
});

let config = SplitConfig {
    model_name: "meta-llama/Llama-3.2-1B-Instruct".to_string(),
    model_path: "./cache".to_string(),
    split_plan,
    output_dir: Some("./shards".to_string()),
};

let result = splitter.split_model(config, "node_001").await?;
println!("切分完成: {} MB", result.shard_size_mb);
```

## 关键点说明

### 1. 元数据格式

元数据 JSON 包含以下字段：

```json
{
  "model_name": "meta-llama/Llama-3.2-1B-Instruct",
  "model_type": "transformer",
  "batch_size": 1,
  "sequence_length": 512,
  "layers": [
    {
      "name": "transformer.h.0.attn.q_proj.weight",
      "shape": [4096, 4096],
      "num_params": 16777216,
      "compute_required": 134.22,
      "layer_type": "attention",
      "dtype": "torch.float32"
    }
  ],
  "total_compute": 1234.56,
  "total_layers": 100
}
```

### 2. 按算力切分算法

Worker 使用贪心算法：
1. 按算力需求从高到低排序所有层
2. 将算力需求高的层分配给算力高的节点
3. 确保每个节点的总算力需求不超过其可用算力

### 3. 切分方案格式

```json
{
  "node_001": {
    "layer_names": ["layer1", "layer2"],
    "total_compute": 100.0,
    "compute_utilization": 0.5
  }
}
```

## 注意事项

1. **Hugging Face Token**: 需要有效的 HF Token 才能上传元数据
2. **元数据仓库**: 需要先在 Hugging Face 上创建公共仓库
3. **Python 依赖**: Rust 模块中的 Python 脚本需要安装 transformers 和 torch
4. **Worker 配置**: 需要配置正确的 METADATA_REPO 变量

## 下一步

1. 完善 Worker 端的节点选择算法（从 D1/DO 获取节点列表）
2. 添加错误处理和重试机制
3. 优化元数据生成速度（并行处理）
4. 添加切分验证机制
