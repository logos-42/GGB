#!/usr/bin/env python3
"""
æ¨¡å‹ä¿¡æ¯æå–å™¨
æ— éœ€åŠ è½½æƒé‡ï¼Œåªæå–æ¨¡å‹å…ƒæ•°æ®å’Œç»“æ„ä¿¡æ¯
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional


def extract_model_info(model_path: Path) -> Dict[str, Any]:
    """æå–æ¨¡å‹åŸºæœ¬ä¿¡æ¯"""
    print(f"æå–æ¨¡å‹ä¿¡æ¯ä»: {model_path}")
    
    if not model_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
    
    # è¯»å–é…ç½®æ–‡ä»¶
    config_file = model_path / "config.json"
    if not config_file.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° config.json æ–‡ä»¶: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # æ£€æŸ¥ safetensors æ–‡ä»¶
    safetensors_file = model_path / "model.safetensors"
    file_size = 0
    if safetensors_file.exists():
        file_size = safetensors_file.stat().st_size
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡ï¼ˆåŸºäºé…ç½®ä¼°ç®—ï¼‰
    hidden_size = config.get('hidden_size', 0)
    num_hidden_layers = config.get('num_hidden_layers', 0)
    vocab_size = config.get('vocab_size', 0)
    intermediate_size = config.get('intermediate_size', 0)
    
    # ç®€åŒ–çš„å‚æ•°ä¼°ç®—
    embedding_params = vocab_size * hidden_size
    attention_params = num_hidden_layers * hidden_size * hidden_size * 3  # Q, K, V
    ffn_params = num_hidden_layers * hidden_size * intermediate_size
    output_params = vocab_size * hidden_size
    
    estimated_params = embedding_params + attention_params + ffn_params + output_params
    
    model_info = {
        "model_name": model_path.name,
        "model_type": config.get('model_type', 'unknown'),
        "architecture": config.get('architectures', ['unknown'])[0],
        "hidden_size": hidden_size,
        "num_layers": num_hidden_layers,
        "num_attention_heads": config.get('num_attention_heads', 0),
        "vocab_size": vocab_size,
        "max_position_embeddings": config.get('max_position_embeddings', 0),
        "dtype": config.get('dtype', 'unknown'),
        "file_size_gb": file_size / (1024**3),
        "estimated_parameters": estimated_params,
        "layer_types": config.get('layer_types', []),
        "config": config
    }
    
    return model_info


def create_model_partitions_info(model_info: Dict[str, Any], num_parts: int = 2) -> List[Dict[str, Any]]:
    """åˆ›å»ºæ¨¡å‹åˆ†åŒºä¿¡æ¯ï¼ˆä¸å®é™…åŠ è½½æƒé‡ï¼‰"""
    print(f"åˆ›å»º {num_parts} ä¸ªåˆ†åŒºçš„ä¿¡æ¯...")
    
    num_layers = model_info['num_layers']
    estimated_params = model_info['estimated_parameters']
    
    partitions = []
    layers_per_part = num_layers // num_parts
    
    for i in range(num_parts):
        start_layer = i * layers_per_part
        end_layer = start_layer + layers_per_part if i < num_parts - 1 else num_layers
        
        # ä¼°ç®—æ¯ä¸ªåˆ†åŒºçš„å‚æ•°æ•°é‡
        part_ratio = (end_layer - start_layer) / num_layers
        part_params = int(estimated_params * part_ratio)
        
        # æ·»åŠ åµŒå…¥å±‚åˆ°ç¬¬ä¸€ä¸ªåˆ†åŒº
        if i == 0:
            embedding_params = model_info['vocab_size'] * model_info['hidden_size']
            part_params += embedding_params
        
        # æ·»åŠ è¾“å‡ºå±‚åˆ°æœ€åä¸€ä¸ªåˆ†åŒº
        if i == num_parts - 1:
            output_params = model_info['vocab_size'] * model_info['hidden_size']
            part_params += output_params
        
        partition_info = {
            "part_id": i,
            "layer_range": [start_layer, end_layer],
            "num_layers": end_layer - start_layer,
            "estimated_params": part_params,
            "estimated_size_gb": (part_params * 4) / (1024**3),  # å‡è®¾ float32
            "description": f"Layers {start_layer}-{end_layer-1}"
        }
        
        partitions.append(partition_info)
    
    return partitions


def save_model_info(model_info: Dict[str, Any], partitions: List[Dict[str, Any]], output_dir: Path):
    """ä¿å­˜æ¨¡å‹ä¿¡æ¯å’Œåˆ†åŒºä¿¡æ¯"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    info_file = output_dir / "model_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    print(f"ä¿å­˜æ¨¡å‹ä¿¡æ¯åˆ°: {info_file}")
    
    # ä¿å­˜åˆ†åŒºä¿¡æ¯
    partitions_file = output_dir / "partitions_info.json"
    with open(partitions_file, 'w', encoding='utf-8') as f:
        json.dump({
            "num_partitions": len(partitions),
            "partitions": partitions
        }, f, indent=2, ensure_ascii=False)
    print(f"ä¿å­˜åˆ†åŒºä¿¡æ¯åˆ°: {partitions_file}")
    
    # åˆ›å»ºåˆ†åŒºå ä½æ–‡ä»¶ï¼ˆç”¨äºåç»­å®é™…æƒé‡åŠ è½½ï¼‰
    for partition in partitions:
        placeholder_file = output_dir / f"partition_{partition['part_id']}_placeholder.json"
        placeholder_data = {
            "part_id": partition["part_id"],
            "layer_range": partition["layer_range"],
            "estimated_params": partition["estimated_params"],
            "status": "placeholder - needs GPU loading",
            "note": "This is a placeholder. Actual weights need to be loaded with GPU support."
        }
        
        with open(placeholder_file, 'w', encoding='utf-8') as f:
            json.dump(placeholder_data, f, indent=2, ensure_ascii=False)
        print(f"åˆ›å»ºåˆ†åŒºå ä½æ–‡ä»¶: {placeholder_file}")
    
    return [info_file, partitions_file] + [output_dir / f"partition_{i['part_id']}_placeholder.json" for i in partitions]


def create_gpu_loading_script(output_dir: Path, model_path: Path):
    """åˆ›å»º GPU ç¯å¢ƒä¸‹çš„æƒé‡åŠ è½½è„šæœ¬"""
    script_content = '''#!/usr/bin/env python3
"""
GPU ç¯å¢ƒä¸‹çš„æ¨¡å‹æƒé‡åŠ è½½è„šæœ¬
åœ¨æœ‰ GPU æ”¯æŒçš„ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬æ¥å®é™…åŠ è½½æƒé‡
"""

import torch
import numpy as np
from safetensors import safe_open
import json
from pathlib import Path

def load_model_weights_gpu(model_path: Path, output_dir: Path, num_parts: int = 2):
    """åœ¨ GPU ç¯å¢ƒä¸­åŠ è½½æ¨¡å‹æƒé‡"""
    print("åœ¨ GPU ç¯å¢ƒä¸­åŠ è½½æ¨¡å‹æƒé‡...")
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("è­¦å‘Š: æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU")
        device = "cpu"
    else:
        device = "cuda"
        print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name()}")
    
    safetensors_file = model_path / "model.safetensors"
    if not safetensors_file.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {safetensors_file}")
    
    # åŠ è½½æ‰€æœ‰æƒé‡
    weights = {}
    with safe_open(safetensors_file, framework="pt") as f:
        for key in f.keys():
            tensor = f.get_tensor(key)
            weights[key] = tensor.to(device)
            print(f"åŠ è½½: {key} - {tensor.shape} - {tensor.dtype}")
    
    # æŒ‰å±‚æ‹†åˆ†æƒé‡
    layers = []
    for name, tensor in weights.items():
        # è½¬æ¢ä¸º float32 å¹¶æ‰å¹³åŒ–
        if tensor.dtype == torch.bfloat16:
            tensor = tensor.float()
        
        flat_params = tensor.flatten().cpu().numpy().astype(np.float32)
        
        layer_info = {
            "name": name,
            "layer_type": str(tensor.dtype),
            "shape": list(tensor.shape),
            "parameters": flat_params.tolist()
        }
        layers.append(layer_info)
    
    # æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†
    total_layers = len(layers)
    layers_per_part = total_layers // num_parts
    
    partitions = []
    for i in range(num_parts):
        start_idx = i * layers_per_part
        end_idx = start_idx + layers_per_part if i < num_parts - 1 else total_layers
        
        part_layers = layers[start_idx:end_idx]
        part_params = sum(len(layer["parameters"]) for layer in part_layers)
        
        partition = {
            "part_id": i,
            "layers": part_layers,
            "total_params": part_params
        }
        partitions.append(partition)
        
        # ä¿å­˜åˆ†åŒº
        output_file = output_dir / f"partition_{i}_weights.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(partition, f, indent=2, ensure_ascii=False)
        
        print(f"ä¿å­˜åˆ†åŒº {i}: {len(part_layers)} å±‚, {part_params:,} å‚æ•°")
    
    print("æƒé‡åŠ è½½å’Œæ‹†åˆ†å®Œæˆ!")
    return partitions

if __name__ == "__main__":
    model_path = Path("''' + str(model_path) + '''")
    output_dir = Path("''' + str(output_dir) + '''")
    
    load_model_weights_gpu(model_path, output_dir)
'''
    
    script_file = output_dir / "load_weights_gpu.py"
    with open(script_file, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"åˆ›å»º GPU åŠ è½½è„šæœ¬: {script_file}")
    return script_file


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æå–æ¨¡å‹ä¿¡æ¯ï¼ˆæ— éœ€åŠ è½½æƒé‡ï¼‰")
    parser.add_argument("model_path", help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--num-parts", type=int, default=2, help="åˆ†åŒºæ•°é‡")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    try:
        model_path = Path(args.model_path)
        output_dir = Path(args.output_dir) if args.output_dir else model_path.parent / "model_info"
        
        # æå–æ¨¡å‹ä¿¡æ¯
        model_info = extract_model_info(model_path)
        
        # åˆ›å»ºåˆ†åŒºä¿¡æ¯
        partitions = create_model_partitions_info(model_info, args.num_parts)
        
        # ä¿å­˜ä¿¡æ¯
        saved_files = save_model_info(model_info, partitions, output_dir)
        
        # åˆ›å»º GPU åŠ è½½è„šæœ¬
        gpu_script = create_gpu_loading_script(output_dir, model_path)
        
        print(f"\nâœ… æ¨¡å‹ä¿¡æ¯æå–å®Œæˆ!")
        print(f"æ¨¡å‹ç±»å‹: {model_info['model_type']}")
        print(f"æ¶æ„: {model_info['architecture']}")
        print(f"å‚æ•°æ•°é‡: {model_info['estimated_parameters']:,}")
        print(f"æ–‡ä»¶å¤§å°: {model_info['file_size_gb']:.2f} GB")
        print(f"æ•°æ®ç±»å‹: {model_info['dtype']}")
        
        print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_path in saved_files + [gpu_script]:
            print(f"  - {file_path}")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
        print(f"1. åœ¨æœ‰ GPU æ”¯æŒçš„ç¯å¢ƒä¸­è¿è¡Œ: python {gpu_script}")
        print(f"2. è¿™å°†å®é™…åŠ è½½æƒé‡å¹¶åˆ›å»ºå®Œæ•´çš„åˆ†åŒºæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
