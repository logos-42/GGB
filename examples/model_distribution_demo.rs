/**
 * æ¨¡å‹åˆ†å‘å®Œæ•´æµç¨‹æ¼”ç¤º
 * ä»ä¸‹è½½åˆ°æ‹†åˆ†çš„å®Œæ•´æµ‹è¯•
 */
use model_downloader::{ModelDownloader, DownloadConfig};
use metadata_generator::{MetadataGenerator, MetadataConfig};
use model_splitter::{ModelSplitter, SplitConfig, SplitPlan};
use std::collections::HashMap;
use tracing::{info, warn, error};
use tracing_subscriber;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();
    
    info!("ğŸš€ å¼€å§‹æ¨¡å‹åˆ†å‘å®Œæ•´æµç¨‹æµ‹è¯•");
    
    // é…ç½®ä¿¡æ¯
    let model_name = "LiquidAI/LFM2.5-1.2B-Thinking";
    let hf_token = std::env::var("HF_TOKEN").ok();
    let node_id = "test_node_001";
    
    // ==================== æ­¥éª¤1: ä¸‹è½½æ¨¡å‹ ====================
    info!("ğŸ“¥ æ­¥éª¤1: ä¸‹è½½æ¨¡å‹...");
    
    let downloader = ModelDownloader::new(hf_token.clone());
    let download_config = DownloadConfig {
        model_name: model_name.to_string(),
        cache_dir: Some("./test_models/rust_download".to_string()),
        hf_token: hf_token.clone(),
    };
    
    let download_result = downloader.download_model(download_config).await
        .map_err(|e| {
            warn!("âš ï¸  ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨å·²å­˜åœ¨çš„æ¨¡å‹: {}", e);
            e
        });
    
    let model_path = match download_result {
        Ok(result) => {
            info!("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {}", result.model_path);
            result.model_path
        }
        Err(_) => {
            // ä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹
            let existing_path = "./test_models/models--LiquidAI--LFM2.5-1.2B-Thinking/snapshots/3c58ec1db4a336594e9ac4ad3fff10fc8aa22d70".to_string();
            info!("ğŸ“‚ ä½¿ç”¨å·²å­˜åœ¨çš„æ¨¡å‹: {}", existing_path);
            existing_path
        }
    };
    
    // ==================== æ­¥éª¤2: ç”Ÿæˆå…ƒæ•°æ® ====================
    info!("ğŸ“Š æ­¥éª¤2: ç”Ÿæˆæ¨¡å‹å…ƒæ•°æ®...");
    
    let generator = MetadataGenerator::new();
    let metadata_config = MetadataConfig {
        model_name: model_name.to_string(),
        model_path: model_path.clone(),
        batch_size: 1,
        sequence_length: 512,
        node_id: Some(node_id.to_string()),
    };
    
    let metadata = generator.generate_metadata(metadata_config).await
        .map_err(|e| {
            error!("âŒ å…ƒæ•°æ®ç”Ÿæˆå¤±è´¥: {}", e);
            e
        })?;
    
    info!("âœ… å…ƒæ•°æ®ç”Ÿæˆå®Œæˆ");
    info!("   - æ¨¡å‹ç±»å‹: {}", metadata.model_type);
    info!("   - æ€»å±‚æ•°: {}", metadata.total_layers);
    info!("   - æ€»è®¡ç®—éœ€æ±‚: {:.2}", metadata.total_compute);
    
    // ä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶
    let metadata_file = format!("./test_models/metadata_{}.json", model_name.replace("/", "_"));
    generator.save_metadata(&metadata, &metadata_file).await?;
    info!("ğŸ“ å…ƒæ•°æ®å·²ä¿å­˜: {}", metadata_file);
    
    // ==================== æ­¥éª¤3: åˆ›å»ºæ‹†åˆ†æ–¹æ¡ˆ ====================
    info!("ğŸ¯ æ­¥éª¤3: åˆ›å»ºæ‹†åˆ†æ–¹æ¡ˆ...");
    
    // æ¨¡æ‹ŸèŠ‚ç‚¹ä¿¡æ¯ï¼ˆå®é™…åº”è¯¥ä»èŠ‚ç‚¹æ³¨å†Œè·å–ï¼‰
    let mut split_plan = HashMap::new();
    
    // èŠ‚ç‚¹1: å¤„ç†å‰åŠéƒ¨åˆ†å±‚
    let node1_layers = metadata.layers.iter()
        .take(metadata.layers.len() / 2)
        .map(|l| l.name.clone())
        .collect();
    
    split_plan.insert(
        "node_001".to_string(),
        SplitPlan {
            node_id: "node_001".to_string(),
            layer_names: node1_layers,
            total_compute: metadata.total_compute / 2.0,
            compute_utilization: 0.8,
        },
    );
    
    // èŠ‚ç‚¹2: å¤„ç†ååŠéƒ¨åˆ†å±‚
    let node2_layers = metadata.layers.iter()
        .skip(metadata.layers.len() / 2)
        .map(|l| l.name.clone())
        .collect();
    
    split_plan.insert(
        "node_002".to_string(),
        SplitPlan {
            node_id: "node_002".to_string(),
            layer_names: node2_layers,
            total_compute: metadata.total_compute / 2.0,
            compute_utilization: 0.8,
        },
    );
    
    info!("âœ… æ‹†åˆ†æ–¹æ¡ˆåˆ›å»ºå®Œæˆ");
    for (node_id, plan) in &split_plan {
        info!("   - {}: {} å±‚, è®¡ç®—éœ€æ±‚: {:.2}", 
              node_id, plan.layer_names.len(), plan.total_compute);
    }
    
    // éªŒè¯æ‹†åˆ†æ–¹æ¡ˆ
    let splitter = ModelSplitter::new();
    let all_layer_names: Vec<String> = metadata.layers.iter()
        .map(|l| l.name.clone())
        .collect();
    
    splitter.validate_split_plan(&all_layer_names, &split_plan)?;
    info!("âœ… æ‹†åˆ†æ–¹æ¡ˆéªŒè¯é€šè¿‡");
    
    // ==================== æ­¥éª¤4: æ‰§è¡Œæ¨¡å‹æ‹†åˆ† ====================
    info!("âš¡ æ­¥éª¤4: æ‰§è¡Œæ¨¡å‹æ‹†åˆ†...");
    
    let split_config = SplitConfig {
        model_name: model_name.to_string(),
        model_path: model_path.clone(),
        split_plan: split_plan.clone(),
        output_dir: Some("./test_models/model_shards".to_string()),
    };
    
    // ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œæ‹†åˆ†
    let mut split_results = Vec::new();
    for node_id in split_plan.keys() {
        info!("ğŸ”§ æ­£åœ¨ä¸ºèŠ‚ç‚¹ {} æ‹†åˆ†æ¨¡å‹...", node_id);
        
        match splitter.split_model(split_config.clone(), node_id).await {
            Ok(result) => {
                info!("âœ… èŠ‚ç‚¹ {} æ‹†åˆ†å®Œæˆ", node_id);
                info!("   - åˆ†ç‰‡è·¯å¾„: {}", result.shard_path);
                info!("   - å‚æ•°æ•°é‡: {}", result.total_params);
                info!("   - åˆ†ç‰‡å¤§å°: {:.2} MB", result.shard_size_mb);
                split_results.push(result);
            }
            Err(e) => {
                error!("âŒ èŠ‚ç‚¹ {} æ‹†åˆ†å¤±è´¥: {}", node_id, e);
                return Err(e);
            }
        }
    }
    
    // ==================== æ­¥éª¤5: ç”Ÿæˆåˆ†å‘æŠ¥å‘Š ====================
    info!("ğŸ“‹ æ­¥éª¤5: ç”Ÿæˆåˆ†å‘æŠ¥å‘Š...");
    
    let total_params: usize = split_results.iter()
        .map(|r| r.total_params)
        .sum();
    
    let total_size_mb: f64 = split_results.iter()
        .map(|r| r.shard_size_mb)
        .sum();
    
    info!("ğŸ‰ æ¨¡å‹åˆ†å‘æµç¨‹å®Œæˆ!");
    info!("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:");
    info!("   - åŸå§‹æ¨¡å‹: {}", model_name);
    info!("   - æ‹†åˆ†èŠ‚ç‚¹æ•°: {}", split_results.len());
    info!("   - æ€»å‚æ•°æ•°é‡: {}", total_params);
    info!("   - æ€»åˆ†ç‰‡å¤§å°: {:.2} MB", total_size_mb);
    info!("   - å¹³å‡æ¯èŠ‚ç‚¹: {:.2} MB", total_size_mb / split_results.len() as f64);
    
    // ä¿å­˜åˆ†å‘æŠ¥å‘Š
    let report = serde_json::json!({
        "model_name": model_name,
        "model_path": model_path,
        "metadata_file": metadata_file,
        "split_plan": split_plan,
        "split_results": split_results,
        "total_params": total_params,
        "total_size_mb": total_size_mb,
        "completed_at": chrono::Utc::now().to_rfc3339()
    });
    
    let report_file = "./test_models/distribution_report.json";
    tokio::fs::write(report_file, serde_json::to_string_pretty(&report)?).await?;
    info!("ğŸ“ åˆ†å‘æŠ¥å‘Šå·²ä¿å­˜: {}", report_file);
    
    info!("ğŸš€ æµ‹è¯•å®Œæˆ! å¯ä»¥å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒäº†!");
    
    Ok(())
}
